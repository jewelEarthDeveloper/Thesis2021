\section{Discussion - Survey}
\label{section:survey_discussion}

\subsection{Threats To Validity}  

\subsubsection{Construct Validity}
The most prominent issue with this survey is whether it is generalisable to the initial questions it was trying to answer.
The question being ``can projectional editing be used to increase the comprehensibility of large Business rules files?''
There is a solid case that this questionnaire did not ask the right questions to answer this hypothesis.
It is difficult to simulate large business rules files in a brief questionnaire.
Comparing the projections to relatively small and non-complex rules collections may not be generalisable to a significant rules collection.

We perhaps had an issue with a mono-operation bias because we only used one tool for measuring - the survey.
We performed many techniques, such as changing orders of questions, measurement, to try and overcome this.
However, all the subjects were still only answering a survey, delivered through the same medium - Survey Monkey.

A possible twist on the response bias may occur from two fronts, where parties may participate in hypothesis guessing and answer questions based on their outcome.
Firstly, two of the respondents were directly known to us, one through work and another through meetings at conferences.
This relationship could have coloured their responses toward a more positive view of our work.
Second, two people who responded were part of the Drools core development team and thus work for JBoss/RedHat.
On top of that, all the people who answered through the Drools consultants mailing list possibly had a direct monetary relationship with RedHat.
This relationship could influence their response towards a more positive view of the status quo.

\subsubsection{Internal Validity}
Whilst we felt we had tried to overcome the apprehension of the subject being judged when answering questions, the fact that one of our questions asked the subject to describe a Drools rule's meaning could overrule all of our previous attempts to assuage that fear.

As with all surveys, the higher the sample size, the greater the chance of validity.
It is difficult to measure the validity of our outcomes due to the relatively small size of our survey.
While 30 respondents are on the low side for using statistical tools to give reasonable responses, statistical validity is also a function of population size.
We requested the size of Drools users from a member of the core Drools development team.  
Unfortunately, that number was not known.
The cross-factor comparisons between the subgroups are of dubious validity as their sample sizes were so small.

\subsubsection{External Validity}
Whilst we feel we reached the right audience of tool users, there was a potential for a geographical selection bias in our population selection technique.
Because a portion of our respondents came from our connections on LinkedIn, and we are from Europe, then there is a particular European bias to our respondents.
Only four of our completed surveys were not from Europe.

There is also a self-selection bias in the sampling. 
As this survey is voluntary, and there is no real personal connection between the subjects and us, then the people who would answer this question are the sort of people who would answer an unsolicited questionnaire.
This bias may affect generalisability to novices, as there was a tendency towards experts in answering the survey.

\subsubsection{Reliability}
Much like our SLR analysis, we also have the credible threat of a single researcher.
All surveys have a subjective nature in their scoring.
Our measurement did not consider cultural differences in question answering between, for example, the Dutch and Italians.

\subsubsection{Repeatability vs Reproducibility}
Repeatability is difficult as the survey is about a particular implementation of a particular tool.
However, our survey could be valid with a different underlying tool, and our population sample selection could work for other researchers.

\subsubsection{Method improvement}
We feel our largest problem was smallness.
We would have tried various measures to increase our sample size if we were to try this again.
These would include contacting people directly within LinkedIn, rather than only those whose email addresses or Twitter handles we could harvest.
Also, we would have followed up on the partially completed questionnaires.
Another option would be to select a more extended period for academic papers from which to harvest addresses.

To overcome our mono-operation bias, we could interview people in person.