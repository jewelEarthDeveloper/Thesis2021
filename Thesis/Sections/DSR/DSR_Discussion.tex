\section{Discussion}
\label{section:dsr_discussion}

We now examine threats to the conclusion, instantiation, internal and external validity of our DSR and its reliability.

\subsection{Threats to Validity} 

\subsubsection{Conclusion Validity}
We felt that the projections we built improved our overview of large rule sets.
However, as we spent a lot of time with the rules as the projections are being built, we could have been influenced by the previous knowledge of the rules.
Thus, we needed to validate our conclusions with others, which chapter \ref{chapter:Survey} attempts to do.

\subsubsection{Instantiation Validity}
As Lukyanenko\cite{Lukyanenko_2014} points out about the validity of DSR, ``executable software systems differ in important ways from ``traditional'' experimental stimuli \dots thus unique challenges inherent in the design of IT artifacts warrant additional attention''.
Thus, here we use his ``Instantiation Validity'', rather than the traditional construct validity.

We were trying to observe whether projectional techniques could help with understanding Drools rules. 
As some of the outcomes could have also been achieved with other, non-projectional, language workbenches, these did not contribute to proving a link between better understanding and projectional editing.
As the tabular projections would not be achievable in parser-based languages, then these could show the direct relationship between projections and understanding.

We offered only a few solutions that could have been created in the instantiation space.

The artefact complexity, with regards to a user having to learn methods of interaction with a projectional editor, may have an impact on the view of understandability.
The nature of software artefacts means that it is challenging to ensure that the projections we show are representative of the projections that can be shown.

Developing working software is a resource hungry activity. 
It took us a long time to get to a good implementation of the Drools-lite language.
As a result of this constraint, we did not create the ideal artefact, with only a few projections accomplished.
This limited supply of alternatives does not allow us to control for confounding effects.

\subsubsection{Internal Validity}
If there was a better understanding, was this the result of the tabular projectional editing?
There is a risk from the interaction of different treatments.
There are many advantages that come from the implementation in MPS, these include the context aware code completion menus.
Some of these side effects, rather than the core concept, could have impacted our understanding.

\subsubsection{External Validity}
Our Drools-Lite language is not a full implementation of the Drools language.
To take in the whole language would have taken more time.
Whether the examples we built would generalise to all the functionality of Drools is not known.

The mono-operation bias of approaching our research question about business rule and projectional editing, by only implementing Drools and only using MPS, could mean that the results do not generalise to other business rules languages and editors built with different language workbenches. 

\subsubsection{Reliability}
Reliability is how the data, and the analysis is dependent on specific researchers.
With DSR studies, that build a working opensource prototype, then the reliability of the data is known.
The code can just be downloaded and run by any interested party.  

The reliability of our analysis, that the projections improved our understanding, is open to effects of a range of cognitive biases.
Together with experimenter expectancies in the design, this could lead to an unreliable conclusion. 
To take out the effect of our biases this we surveyed others, as described in chapter \ref{chapter:Survey}.
