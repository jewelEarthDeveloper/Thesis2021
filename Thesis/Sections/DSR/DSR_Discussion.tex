\section{Discussion}
\label{section:dsr_discussion}

We now examine threats to the conclusion, instantiation, internal and external validity of our DSR and its reliability.

\subsection{Threats to Validity} 

\subsubsection{Conclusion Validity}
We felt that the projections we built improved our overview of large rule sets.
However, as we spent a lot of time with the rules as the projections are being built, we could have been influenced by the previous knowledge of the rules.
Thus, we needed to validate our conclusions with others, which Chapter \ref{chapter:Survey} attempts to do.

\subsubsection{Instantiation Validity}
As Lukyanenko\cite{Lukyanenko_2014} points out about the validity of DSR, ``executable software systems differ in important ways from ``traditional'' experimental stimuli \dots thus unique challenges inherent in the design of IT artefacts warrant additional attention''.
Thus, here we use his ``Instantiation Validity'' rather than the traditional construct validity.

We were trying to observe whether projectional techniques could help with understanding Drools rules. 
As non-projectional language workbenches could achieve some of the outcomes, these did not prove a link between better understanding and projectional editing.
As the tabular projections would not be achievable in parser-based languages, these could show the direct relationship between projections and understanding.

We offered only a few solutions possible in the instantiation space.
Given more time, we could have provided more candidates to improve understanding.

The artefact complexity, with regards to a user having to learn methods of interaction with a projectional editor, may impact the view of understandability.
The nature of software artefacts means that it is challenging to ensure that the projections we show represent the possible projections.

Developing working software is a resource-hungry activity. 
It took us a long time to get to an adequate implementation of the Drools-lite language.
As a result of this constraint, we did not create the ideal artefact, with only a few projections accomplished.
This limited supply of alternatives does not allow us to control for confounding effects.

\subsubsection{Internal Validity}
Was the outcome a result of the treatment?
By which we mean, if there was a better understanding of the code, was this a consequence of the projectional editing or some other factor?
There is a risk from the interaction of different treatments impacting the outcome.
Many advantages come from the implementation in MPS.
These include the context-aware code completion menus.
Some of the side effects of MPS, rather than the core projectional concept, could have impacted the change in understandability.

\subsubsection{External Validity}
Our Drools-Lite language is not a full implementation of the Drools language.
To take in the whole language would have taken more time.
Whether the examples we built would generalise to all the functionality of Drools is not known.

The mono-operation bias of approaching our research question about business rules and projectional editing by only implementing Drools and only using MPS could mean that the results do not generalise to other business rules languages and editors built with different language workbenches. 

\subsubsection{Reliability}
Reliability is how the data and the analysis is dependent on specific researchers.
With DSR studies that build a working opensource prototype, then the reliability of the data is known.
The code can just be downloaded and run by any interested party.  

The reliability of our analysis that the projections improved our understanding is open to the effects of a range of cognitive biases. 
Together with experimenter expectancies in the design, this could lead to an unreliable conclusion. 
To mitigate the effect of our biases, we surveyed others, as described in Chapter \ref{chapter:Survey}.
