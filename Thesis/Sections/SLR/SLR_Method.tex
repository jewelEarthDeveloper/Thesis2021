\section{Method}
\label{section:slr_method}

To answer the first Research Question, ``what is the current state of Projectional Editing?'' we conducted a systematic literature review.
Hereafter, we describe the method we undertook. 
We followed Kitchenham's\cite{kitchenham2015evidence} advice on systematic literature review protocol validation to carry out this review.

\subsection{Motivation}
The motivation that preceded this research was a requirement to understand if projectional editing was an idea that was worth investigating.
Our background research showed an interest in the precursors to projectional editing in the late '70s through to the mid-'80s.
Outside of academia, interest arose in the mid-'90s following Charles Simonyi's treaties on Intentional programming.
However, Simonyi's call to arms did not lead to a swell in academic research as his company's product, Intentional Domain Workbench, was a closed commercial product.
Conversely, after JetBrains's opensource Meta Programming System (MPS) release, in the late 2000s, there was a flurry of papers on the subject.

Is there a need for a study of this topic? 
We believe, at least in the microcosm of this master's project, it is helpful to know whether we are researching in a dying or vibrant area.

There does not seem to be any recent SLRs specifically about projectional editing.
This study is not extending any previously conducted SLR.
Although there exist literature surveys and mapping studies in some adjacent fields, we found no SLRs.
Thus, we believe it may be helpful for those in the language engineering research community to bring together all current research about projectional editing in one place.

\subsection{Research Question}
We try to answer the question "What is the current state of Projectional Editing?" with an SLR. 
We have broken this question into three sub-questions.

\begin{itemize}
    \setlength\itemsep{0em}
    \item \textbf{Sub Question 1} ``Is there any current research in the area of projectional editing?''
    \item \textbf{Sub Question 2} ``Which tools are currently being used for research?''
    \item \textbf{Sub Question 3} ``What is the sentiment in papers currently discussing projectional editing?''
\end{itemize}


\subsection{Search Strategy}

The search process is automated as SLRs require a high level of completeness, which one cannot effectively achieve manually.
Our first major decision was whether to engage in creating a quasi-gold standard as advised by Zhang\cite{Zhang_2011}.
Zhang noted that the ad-hoc nature of search strategies in SLRs has limitations.
We executed a preliminary ad-hoc search to try and ascertain the extent of the research space.
After satisfying that the research space was small enough, we decided against using the Quasi-Gold standard, which was overkill for our requirements.

The search terms we landed on were as follows:
\begin{lstlisting}[frame=none]
     ``PROJECTIONAL EDITING'' 
      OR 
     ``PROJECTIONAL EDITOR'' 
\end{lstlisting}

We adjusted these search terms to fit the query syntax of the various search engines.

As most research search engines offer date ranges, we also used the date range to eliminate unnecessary papers at the automated search stage to save the effort of excluding them later.
In our research question, we are specifically looking at the current state of projectional editing.
A restriction of many research search engines is that they define date ranges in whole years.
When designing our search strategy, it was near the beginning of 2021.
We concluded that only including papers from 2021 would be too small a search space. 
Therefore, we set our date range to be from the beginning of 2020 to the present.
For the sake of reproducibility, we advise the removal of any papers after 31st July 2021.

We show the search engines we used in table \ref{table:searchEngines}.

\begin{table}[h]
    \begin{center}         
        \begin{tabular}{|l||l|}
            \hline
            ACM digital library       & Google Scholar       \\
            \hline
            BASE                      & CORE                 \\
            \hline  
            IEEE Xplore               & ISI Web of Science   \\
            \hline  
            Microsoft Academic        & Science.gov          \\
            \hline  
            Wiley InterScience        & SCOPUS               \\
            \hline  
            Semantic Scholar          & SpringerLink         \\
            \hline  
        \end{tabular}
    \end{center}
    \caption{Search engines used}
    \label{table:searchEngines}
\end{table}

Once we have filtered the automated search through the criteria of the selection stage, we will use that as our starting set for snowballing.
We will do all our filtering before we do any quality assessments, as we feel that excluding papers from snowballing based on the quality of the primary study would artificially limit the network of potential papers.
Our snowballing procedure shall follow the advice of Wohin\cite{Wohlin_2014}.
Snowballing is a technique for finding related papers using the reference lists in our starting set and applying the same selection criteria.

Where possible, we will get the forward snowballing papers from the ``cited by'' functionality of Google Scholar.
Because of the range of the search being ``to present'', all papers that cite the target paper will fall within our criteria.
For backward snowballing, we will manually filter the bibliography section of the selected papers, selecting any paper published in 2020 or 2021

After gathering all the papers from the forward and backwards snowballing, we will apply the selection criteria again.
The snowballing process will recursively iterate until there are no new papers.
The papers accepted in each iteration will form the basis for the following stage - the quality assessment of the primary studies.

After the final iteration, as a final step, the selected papers will have a deeper scan. 
This deep scan ensures that the papers selected in our initial scan meet our inclusion criteria before moving on to the quality assessment.

\subsection{Study Selection}

The inclusion criteria are:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Studies are about or mention projectional editing or one of its synonyms.
    \item The study published date is in the period 2020-2021.
\end{itemize}

\noindent
The exclusion criteria are:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Books and grey literature.
    \item Not in English.
    \item Full text unavailable.
    \item Papers with severe issues with grammar or vocabulary.
    \item A duplicated paper.
    \item The primary study is in a previously selected paper.
\end{itemize}

If multiple papers look at the same study with different approaches, we aggregate the data during the synthesis stage.

As a lone researcher, we must be aware of bias in positively including relevant papers and excluding irrelevant papers.
We will follow Kitchenham's suggestions to overcome such bias:
\begin{itemize}
    \setlength\itemsep{0em}
    \item Test-retest 
    \begin{itemize}
        \setlength\itemsep{0em}
        \item We will assess the papers once (on title abstract and keywords) against the inclusion and exclusion criteria.
        \item Save all the suggested results.
        \item Assess the papers again three days later in a different order to the first.  
    \end{itemize}
    \item If there are disagreements, we will use Cohen's\cite{Cohen_1960} Kappa agreement statistic to see if refinement of the process is needed.
\end{itemize} 

If our searches appear too large for a lone researcher, we will turn to text mining.
We will be cautious about using this.
O'Mara-Eves et al.'s\cite{OMara-Eves_2015} systematic review of text mining in systematic reviews recommends using this for prioritisation but finds that for exclusion screening, although promising, it is not yet proven.

An SLR is interested in studies rather than papers.
There is a many-to-many relation between papers and studies.
We will review the selected papers to note when this has happened in our results to make sure studies do not get over or undercounted.

\subsection{Quality of Primary Studies}
To discover explanatory reasons for why there may be differences in study results and to weigh how valuable specific studies are, we will assess the quality of the selected studies.

To avoid a ``Results Section bias'', we will be operating a results-blind quality assessment.
We base our study quality on the methods section of the papers only.
However, this bias is still a threat because the abstract, which we will read, summarises the results.
The study quality will not be measured until after the selection process is complete, though it will, in part, occur before the selection re-test process.

We will use checklists from the Center for Evidence-Based Management, found in Appendix \ref{appendix:QualityAssesmentChecklist}, to conduct the quality assessments. 
These checklists address general scientific research.

In software engineering, many studies fall under what Gregor\cite{gregor2006nature}, in ``A Taxonomy of Theory Types in Information Systems Research'', calls ``Type V: Theory for Design and Action''.
These types of studies are also known as Design Science Research (DSR).
Staron\cite{Staron_2019} describes DSR as ``\dots a methodology which we can place close to the engineering, technical areas of software engineering. 
Design science is the design and investigation of artefacts in context. 
A design science research project, therefore, seeks to solve an empirical or industrial problem, with the help of an artefact. 
It recognises two contributions â€” the construction and evaluation of the artefact and the development of new knowledge.''

The checklists do not address this type of research well.
Despite investigating how other SLRs conducted quality assessments of DSR studies, we did not find a good checklist for DSRs.
Therefore, we will continue with the checklists shown in the appendix, using the checklists meant for Case Studies for the DSR research papers.
We will take this into account before dismissing results of this type based on their quality score.

As a lone researcher will carry out this study, there is no need to have a process for disagreements between researchers.

We use the quality assessment checklist to weed out the biases of selection, performance, detection, exclusion, and other threats to the validity of the studies under test.


\subsection{Data Extraction}
\label{section:dataExtraction}
No data extraction will be necessary for the first sub-question,  ``Is there current research in the area of projectional editing?''.
The existence of papers with verified primary studies either into projectional editing theory or its practical use will be enough to answer the question.

For the question of ``What tools are currently being used for research?'' we shall note each tool discussed mentioned explicitly in the study.

Finally, for the sentiment, we shall pass each paragraph of the introduction, the discussion, the conclusion, and any other sections that mention projectional editing or tools through a sentiment analyser, noting its sentiment score.
The sentiment analysis tool we shall use is Microsoft Azure Cognitive Services Text Analytics.
We show the code to carry out this task in listing \ref{listing:text_analytics}.

\noindent\begin{minipage}[h]{\textwidth}
    \begin{lstlisting}[language=Python, numbers=left, caption=Text Analytics code, captionpos=b, label=listing:text_analytics, breaklines=true]

    from azure.core.credentials import AzureKeyCredential
    from azure.ai.textanalytics import TextAnalyticsClient
    
    endpoint = "REPLACE_WITH_CORRECT_ENDPOINT"
    key = "REPLACE_WITH_CORRECT_KEY"
    
    text_analytics_client = TextAnalyticsClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    
    inputfiles = [[ARRAY_OF_FILES_TO_BE_ANALYSED]]
    
    with open('/content/sample_data/sentiment/output_all.txt','a') as outf:
    for sections in inputfiles:
        for section in sections:
        print("Section: {}".format(section),file=outf)
        f = open('/content/sample_data/sentiment/'+section)
        content = f.readlines()
        # for brevity an optimization to deal with 10 document limit is removed
        if len(content) != 0:
            result = text_analytics_client.analyze_sentiment(content, show_opinion_mining=True)
            docs = [doc for doc in result if not doc.is_error]
            for idx, doc in enumerate(docs):
            print("sentiment: {}".format(doc.sentiment),file=outf)
            print("Document text: {}".format(content[idx]),file=outf)
    \end{lstlisting}
\end{minipage}

We will gather this data in tables with the categories shown in table \ref{table:Data_Extraction_Form}.

\begin{table}[h]
	\centering
	\begin{tabular}{|c | l | l | c |} 
		\hline
		\#& Data Type           & Description                                          & RQ     \\ \hline
		\hline
        1 & Study ID            & Unique identifier for the study                      &        \\ \hline
        2 & Title of Study      & The paper name                                       &        \\ \hline
        3 & Year of Publication & It will be either 2020 or 2021                       &        \\ \hline
        4 & Author(s) Names     &                                                      &        \\ \hline
        5 & Source of Study     & Name Of Online Database/ Digital Library             &        \\ \hline
        6 & Type of Study       & Experiment/Case Study/Survey/DSR                     &        \\ \hline
        7 & Name of Venue       & publishing Journal/Conference                        &        \\ \hline
        8 & Tools in Study      & A list of the tools used                             & RQ 1.2 \\ \hline
        9 & Sentiment           & The sentiment scores from appropriate paragraphs     & RQ 1.3 \\ \hline		
	\end{tabular}	
	\caption{Data extraction form}
    \label{table:Data_Extraction_Form}
\end{table}

\subsection{Data Aggregation and Synthesis}
Kitchenham\cite{kitchenham2015evidence} explained that primary studies would be too heterogeneous for any statistical analysis in software engineering. 
Synthesising outcomes from multiple methods will be complex.
Thus, our synthesis will take a narrative approach.

Narrative synthesis tells a story of the who, how, and why of the success or otherwise of the research.
For DSR research, the focus will be on what will help or hinder the adoption of the implementations.
It will also examine how reliable the results are and the relationships between the studies.

