\section{Discussion}
\label{section:slr_discussion}

We now examine threats to the construct, the internal and external validity of our SLR, its reliability, and areas of improvement.

 
\subsection{Threats To Validity}  
As discussed in their tertiary study of SLRs, da Silva et al.\cite{DaSilvaFabioQ.B2011Syos}, one of the main problems of SLRs in Software engineering is a focus on practice and not experimentation.
Because of the nature of the subject area, we will be making this same shortfall. 
We feel that we have fully addressed their other concerns of SLRs not assessing the quality of our primary studies, bad integration and lack of guidelines.

As with all SLRs, the main threats to validity are an incomplete set of studies due to an insufficient search strategy, researcher bias in paper selection and inaccuracy in data extraction.
In our study quality assessment, we used Runeson et al.'s\cite{runeson2009guidelines} four suggested limitations of studies, namely construct validity, internal validity, external validity, and reliability.
It is only fair that we point this towards our study.

\subsubsection{Construct Validity}
Regarding construct validity, i.e., whether our research questions match the research subjects methods and measures, whilst no measurement system is perfect, some are much further from perfect than others.

For the construct to be valid, we need to present the best available evidence.
The nature and modernity of projectional editing might mean plenty of good evidence is available in grey literature and industrial articles. 
This under-representation of actual but non-academic studies could lead to a false positive or negative for some of the questions, leading to errors in recommendations.

There may be circumstances that influence the best evidence, such as who is funding the study. 
Is a researcher working or consulting at a projectional editing product supplier, and will this skew results.

Are some projectional editors being ignored because of the preference for English papers only?
The focus on English language papers might be biased against projectional editors aimed at non-English speaking markets.

The use of the sentiment analysis tool may have been inappropriate.
Scientific papers may not lend themselves to sentiment analysis in general.
The Azure sentiment analysis service may not be appropriate for scientific papers.

\subsubsection{Internal Validity}
Internal validity, or the causal relationships, does one factor cause an effect or are both factors influenced by something unseen.

An incomplete search term may have lead to selection bias.  
Using ``projectional editing'' and ``projectional editor'' may have led us into a small corner of this field.
If one tool uses this term and others use other terms to describe a similar approach, these search terms may misrepresent the field.
Other tools or projects could use different terms, such as ``language orientated programming'' or more antiquated terms like ``structured programming'' or ``syntax-directed editing''.

One causal relationship that could have influenced the outcome was that the book ``Domain-Specific Languages in Practice with JetBrains MPS'' was published right at the end of our selection period.
Of the 11 papers published in this book, 7 made it into our final paper selection.
As this book is all studies involving using MPS, then this skews the data towards MPS.

We also had an error with the identification of primary studies. 
Initially, we took the word of the paper when it said it had done a case study, but frequently, that just meant trying their code out on a current problem rather than a case study in the academic sense.

A final threat to internal validity was that no expert in either Drools or projectional editing evaluated the conclusions we drew.

\subsubsection{External Validity}
External validity is the ability to generalise the findings.

The prevalence of the DSR methodology in software engineering is why these results are impossible to generalise.
The ``primary study'' data in these cases are often just the source code.

Another threat to validity that this is a good representation of the field is our restricted timespan of the search.
However, as the scope of the question was to examine the current state, we still feel that this restriction is necessary and informative.

\subsubsection{Reliability}
Reliability is how the data and the analysis is dependant on specific researchers.
Here we are presented with a very credible threat in that a single researcher carried out this research.
Under particular threat from the single-researcher bias were the quality assessments. 

Whilst measures were put in place to try and mitigate this, the reliance on a single person's judgement of the underlying studies leaves the door to bias wide open.
Another threat is the use of narrative review. 
This review type can be subjective and, therefore, difficult to reproduce.

As mentioned in the results, we were not happy with any of the paper quality assessments for DSR studies.
Using an inappropriate quality assessment tool was instrumental in the almost complete failure of the quality assessments.


\subsubsection{Repeatability Vs Reproducibility}
Greenhalgh et al.\cite{GreenhalghTrisha2005Eaeo} in their review of where papers come from in SLRs, found that 24\% of papers come from "personal knowledge or personal contact".
For the sake of reproducibility, we decided not to hunt down relevant papers from knowledgeable people in the field, as this would require anyone trying to reproduce this study to ask the same people at the same knowledge level as us.

\subsubsection{Method Improvement}
An area we would improve is sentiment analysis.
Our technique was flawed.
To avoid the risk of bias through us cherry picking paragraphs, we were very coarse-grained in our input selection criteria.
For all papers, we chose the introduction, the conclusion and then any section that discussed anything to do with projectional editing.

The problem is that sometimes the sections would run for paragraphs, with only one or two being about projectional editing.
Frequently, especially with papers involving MPS, the paper was focused on a problem and using MPS to solve it.
Thus, the introductions and conclusions would occasionally barely mention projectional editing.

If we were to do this again, we would take a more fine-grained approach as we feel the cherry-picking risk is less significant than the noise from the unrelated text.
