\section{Discussion - Survey}

\subsection{Threats to Validity}  

\subsubsection{Construct Validity}
The most prominent issue with this survey is whether it is generalizable to the initial questions it was trying to answer.
The question being can projectional editing be used to increase the comprehensibility of large Business rules files.
There is a strong case to be made that this questionnaire did not ask the right sort of questions to answer this hypothesis.
It is difficult to simulate large business rules files in a brief questionnaire.
Comparing the projections to relatively small and non complex rules collections may not be generalizable to a large rules collection.

We perhaps had an issue with a mono-operation bias, because we only used one tool for measuring - the survey.
We performed many techniques, such as changing orders of questions, measurement, rulesets, to try and overcome this.
However, all the subjects were still just answering a survey, delivered through the same medium - Survey Monkey.

A possible relationship bias may occur from two fronts, where parties may participate in hypothesis guessing and answer questions based on their outcome.
Firstly, two of the respondents were directly known to us, one through work and another through meetings at conferences.
This could have colored their responses toward a more positive view of our work.
Second, two people who responded were part of the Drools core development team, and thus work for JBoss/RedHat.
On top of that all the people who answered through the Drools consultants mailing list possibly had a direct monetary relationship with RedHat.
This could influence their response towards a more positive view of the status quo.

\subsubsection{Internal Validity}
Whilst we felt we had tried to overcome the apprehension of being judged when answering questions, the fact that one of our questions ask the subject to describe a drools rule's meaning, could overrule all of our previous attempts to assuage that fear.

As with all surveys, the higher the sample size the greater the chance of validity.
It is difficult to measure the validity of our outcomes, due to the relatively small size of our survey.
Whilst 30 respondents is on the low side for using statistical tools to give reasonable responses, it is also dependant on size of population.
We requested the size of the population of Drools users from a member of the core Drools development team, but apparently that number was not known.
The cross-factor comparisons between the subgroups within is of dubious validity as their sample sizes were so small.

\subsubsection{External Validity}
Whilst we feel we reached the right audience of tool users, there was a potential for a geographical selection bias in our population selection technique.
Because a portion of our respondents came from our connections on LinkedIn, and we are from Europe, then there is a particular European bias to our respondents.
Only four of our completed surveys were not from Europe, three if using the Eurovision song contest definition of Europe.

There is also a self-selection bias in the sampling. 
As this survey is voluntary, and there is no real personal connection between the subjects and us, then the people who would answer this question are the sort of people who would answer an unsolicited questionnaire.
This bias may effect generalizability to novices, as there was a tendency towards experts in answering the survey.

\subsubsection{Reliability}
Much like our SLR analysis, the we are here also presented with the credible threat of a single researcher.
All surveys have a subjective nature in their scoring.
Our measurement did not take into account cultural differences between, for example, the blunt Dutch and the polite Italians.

\subsubsection{Repeatability vs Reproducibility}
Repeatability is hampered by the specification of the tool in use.
However, we feel that our survey could be valid with a different underlying tool and our population selection could work for other researchers.

\subsubsection{Method improvement}
We feel our largest problem was smallness.
If we were to try this again we would have tried various measures to increase our population size.
These would include contacting people directly within LinkedIn, rather than only those whose email or twitter address we could harvest.
Also we would have followed up the partially completed questionnaires.
Another option would be a larger time period for academic papers.

To overcome our mono-operation bias, we could interview people in person.